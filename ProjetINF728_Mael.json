{"paragraphs":[{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545299129103_-2108137138","id":"20181220-094529_741989106","dateCreated":"2018-12-20T09:45:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1451","text":"%md\n\n## Imports","dateUpdated":"2018-12-20T09:46:39+0000","dateFinished":"2018-12-20T09:46:39+0000","dateStarted":"2018-12-20T09:46:39+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Imports</h2>\n</div>"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298460617_-1528191415","id":"20181220-093420_248996215","dateCreated":"2018-12-20T09:34:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:240","text":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection \nimport org.apache.spark.sql.functions._","dateUpdated":"2018-12-20T09:34:35+0000","dateFinished":"2018-12-20T09:35:12+0000","dateStarted":"2018-12-20T09:34:35+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.URL\nimport java.io.File\nimport java.io.File\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.net.HttpURLConnection\nimport org.apache.spark.sql.functions._\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545299205045_1774070617","id":"20181220-094645_1016288376","dateCreated":"2018-12-20T09:46:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1618","text":"%md\n ## Download datas","dateUpdated":"2018-12-20T09:47:01+0000","dateFinished":"2018-12-20T09:47:01+0000","dateStarted":"2018-12-20T09:47:01+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Download datas</h2>\n</div>"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298475568_-1440913049","id":"20181220-093435_1203523695","dateCreated":"2018-12-20T09:34:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:323","text":"def fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n    val url = new URL(urlOfFileToDownload)\n    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n    connection.setConnectTimeout(5000)\n    connection.setReadTimeout(5000)\n    connection.connect()\n\n    if (connection.getResponseCode >= 400)\n        println(\"error\")\n    else\n        url #> new File(fileName) !!\n}","dateUpdated":"2018-12-20T09:36:06+0000","dateFinished":"2018-12-20T09:36:08+0000","dateStarted":"2018-12-20T09:36:06+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one feature warning; re-run with -feature for details\nfileDownloader: (urlOfFileToDownload: String, fileName: String)Any\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298566523_1809854456","id":"20181220-093606_2031630974","dateCreated":"2018-12-20T09:36:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:426","text":"fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"/mnt/tmp/masterfilelist.txt\") // save the list file to the Spark Master","dateUpdated":"2018-12-20T09:36:19+0000","dateFinished":"2018-12-20T09:36:22+0000","dateStarted":"2018-12-20T09:36:19+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res6: Any = \"\"\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545299302890_1175461358","id":"20181220-094822_43901812","dateCreated":"2018-12-20T09:48:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1711","text":"%md\n## Connect to S3 bucket","dateUpdated":"2018-12-20T09:50:26+0000","dateFinished":"2018-12-20T09:50:26+0000","dateStarted":"2018-12-20T09:50:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Connect to S3 bucket</h2>\n</div>"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298579770_658728855","id":"20181220-093619_1608793220","dateCreated":"2018-12-20T09:36:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:529","text":"import com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\n    \nval AWS_ID = \"AKIAJIKEYNJBYTBQ44IA\"\nval AWS_KEY = \"IJ5ogQ1HRASverlSAKgs0gggwgev4I0t43gkjrrT\"\nval awsClient = new AmazonS3Client(new BasicAWSCredentials(AWS_ID, AWS_KEY))\n\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID) // mettre votre ID du fichier credentials.csv","dateUpdated":"2018-12-20T09:36:35+0000","dateFinished":"2018-12-20T09:36:37+0000","dateStarted":"2018-12-20T09:36:35+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nAWS_ID: String = AKIAJIKEYNJBYTBQ44IA\nAWS_KEY: String = IJ5ogQ1HRASverlSAKgs0gggwgev4I0t43gkjrrT\nwarning: there was one deprecation warning; re-run with -deprecation for details\nawsClient: com.amazonaws.services.s3.AmazonS3Client = com.amazonaws.services.s3.AmazonS3Client@cacc51d\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298595567_-162863836","id":"20181220-093635_1188744926","dateCreated":"2018-12-20T09:36:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:632","text":"sc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY) // mettre votre secret du fichier credentials.csv\n\nawsClient.putObject(\"fabien-mael-telecom-gdelt2018\", \"masterfilelist.txt\", new File( \"/mnt/tmp/masterfilelist.txt\") )","dateUpdated":"2018-12-20T09:36:49+0000","dateFinished":"2018-12-20T09:36:52+0000","dateStarted":"2018-12-20T09:36:49+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res11: com.amazonaws.services.s3.model.PutObjectResult = com.amazonaws.services.s3.model.PutObjectResult@5cc28652\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545299431641_-1797436336","id":"20181220-095031_1161999018","dateCreated":"2018-12-20T09:50:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1816","text":"%md\n## Create SQL interpreter","dateUpdated":"2018-12-20T09:50:56+0000","dateFinished":"2018-12-20T09:50:56+0000","dateStarted":"2018-12-20T09:50:56+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Create SQL interpreter</h2>\n</div>"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298609277_-1862088921","id":"20181220-093649_403658488","dateCreated":"2018-12-20T09:36:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:735","text":"import org.apache.spark.sql.SQLContext\n\nval sqlContext = new SQLContext(sc)\nval filesDF = sqlContext.read.\n                    option(\"delimiter\",\" \").\n                    option(\"infer_schema\",\"true\").\n                    csv(\"s3a://fabien-mael-telecom-gdelt2018/masterfilelist.txt\").\n                    withColumnRenamed(\"_c0\",\"size\").\n                    withColumnRenamed(\"_c1\",\"hash\").\n                    withColumnRenamed(\"_c2\",\"url\").\n                    cache","dateUpdated":"2018-12-20T09:37:00+0000","dateFinished":"2018-12-20T09:37:25+0000","dateStarted":"2018-12-20T09:37:00+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.SQLContext\nwarning: there was one deprecation warning; re-run with -deprecation for details\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@62fc4931\nfilesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-84-228.ec2.internal:4040/jobs/job?id=0"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298620674_2105295382","id":"20181220-093700_589487055","dateCreated":"2018-12-20T09:37:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:838","text":"filesDF.show(false)","dateUpdated":"2018-12-20T09:37:40+0000","dateFinished":"2018-12-20T09:37:43+0000","dateStarted":"2018-12-20T09:37:40+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+--------------------------------+--------------------------------------------------------------------+\n|size    |hash                            |url                                                                 |\n+--------+--------------------------------+--------------------------------------------------------------------+\n|150383  |297a16b493de7cf6ca809a7cc31d0b93|http://data.gdeltproject.org/gdeltv2/20150218230000.export.CSV.zip  |\n|318084  |bb27f78ba45f69a17ea6ed7755e9f8ff|http://data.gdeltproject.org/gdeltv2/20150218230000.mentions.CSV.zip|\n|10768507|ea8dde0beb0ba98810a92db068c0ce99|http://data.gdeltproject.org/gdeltv2/20150218230000.gkg.csv.zip     |\n|149211  |2a91041d7e72b0fc6a629e2ff867b240|http://data.gdeltproject.org/gdeltv2/20150218231500.export.CSV.zip  |\n|339037  |dec3f427076b716a8112b9086c342523|http://data.gdeltproject.org/gdeltv2/20150218231500.mentions.CSV.zip|\n|10269336|2f1a504a3c4558694ade0442e9a5ae6f|http://data.gdeltproject.org/gdeltv2/20150218231500.gkg.csv.zip     |\n|149723  |12268e821823aae2da90882621feda18|http://data.gdeltproject.org/gdeltv2/20150218233000.export.CSV.zip  |\n|357229  |744acad14559f2781a8db67715d63872|http://data.gdeltproject.org/gdeltv2/20150218233000.mentions.CSV.zip|\n|11279827|66b03e2efd7d51dabf916b1666910053|http://data.gdeltproject.org/gdeltv2/20150218233000.gkg.csv.zip     |\n|158842  |a5298ce3c6df1a8a759c61b5c0b6f8bb|http://data.gdeltproject.org/gdeltv2/20150218234500.export.CSV.zip  |\n|374528  |dd322c888f28311aca2c735468405551|http://data.gdeltproject.org/gdeltv2/20150218234500.mentions.CSV.zip|\n|11212939|cd20f295649b214dd16666ca451b9994|http://data.gdeltproject.org/gdeltv2/20150218234500.gkg.csv.zip     |\n|362610  |c4268d558bb22c02b3c132c17818c68b|http://data.gdeltproject.org/gdeltv2/20150219000000.export.CSV.zip  |\n|287807  |e7f464a7a451ad2af6e9c8fa24f0ccea|http://data.gdeltproject.org/gdeltv2/20150219000000.mentions.CSV.zip|\n|9728953 |8f4b26e134bd6605cce2d32e92e5d3d7|http://data.gdeltproject.org/gdeltv2/20150219000000.gkg.csv.zip     |\n|251605  |7685a6c71f010918f3be0d4ed2be977e|http://data.gdeltproject.org/gdeltv2/20150219001500.export.CSV.zip  |\n|263793  |e23ee65a60a1577dc74b979a54da406e|http://data.gdeltproject.org/gdeltv2/20150219001500.mentions.CSV.zip|\n|9459370 |6031464dfdcb331551d491916d400c18|http://data.gdeltproject.org/gdeltv2/20150219001500.gkg.csv.zip     |\n|255259  |f41066efb05d4024fca9dc1c2c6b9112|http://data.gdeltproject.org/gdeltv2/20150219003000.export.CSV.zip  |\n|308019  |061133d1efd29c66c7ecba0d52063927|http://data.gdeltproject.org/gdeltv2/20150219003000.mentions.CSV.zip|\n+--------+--------------------------------+--------------------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-84-228.ec2.internal:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545299472225_1001599343","id":"20181220-095112_1075107660","dateCreated":"2018-12-20T09:51:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1909","text":"%md \n## Focus on a single day","dateUpdated":"2018-12-20T09:51:26+0000","dateFinished":"2018-12-20T09:51:26+0000","dateStarted":"2018-12-20T09:51:26+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Focus on a single day</h2>\n</div>"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298660938_-558709618","id":"20181220-093740_2135968653","dateCreated":"2018-12-20T09:37:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:954","text":"val sampleDF = filesDF.filter(col(\"url\").contains(\"/20181201\")).cache","dateUpdated":"2018-12-20T09:39:41+0000","dateFinished":"2018-12-20T09:39:41+0000","dateStarted":"2018-12-20T09:39:41+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sampleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298781194_989297217","id":"20181220-093941_467682395","dateCreated":"2018-12-20T09:39:41+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1073","text":"object AwsClient{\n    val s3 = new AmazonS3Client(new BasicAWSCredentials(AWS_ID, AWS_KEY))\n}\n\n\nsampleDF.select(\"url\").repartition(100).foreach( r=> {\n            val URL = r.getAs[String](0)\n            val fileName = r.getAs[String](0).split(\"/\").last\n            val dir = \"/mnt/tmp/\"\n            val localFileName = dir + fileName\n            fileDownloader(URL,  localFileName)\n            val localFile = new File(localFileName)\n            AwsClient.s3.putObject(\"fabien-mael-telecom-gdelt2018\", fileName, localFile )\n            localFile.delete()\n            \n})","dateUpdated":"2018-12-20T09:40:29+0000"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298829497_1341808708","id":"20181220-094029_1504069845","dateCreated":"2018-12-20T09:40:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1173","text":"sc.hadoopConfiguration.set(\"fs.s3a.access.key\", \"AKIAJIKEYNJBYTBQ44IA\") // mettre votre ID du fichier credentials.csv\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", \"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\") // mettre votre secret du fichier credentials.csv","dateUpdated":"2018-12-20T09:42:23+0000","dateFinished":"2018-12-20T09:42:24+0000","dateStarted":"2018-12-20T09:42:23+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545298943927_-1702529020","id":"20181220-094223_861956335","dateCreated":"2018-12-20T09:42:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1248","text":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n// 20181201000000.export.CSV.zip\nval textRDD = sc.binaryFiles(\"s3a://fabien-mael-telecom-gdelt2018/20181201[0-9]*.export.CSV.zip\"). // charger quelques fichers via une regex\n   flatMap {  // decompresser les fichiers\n       case (name: String, content: PortableDataStream) =>\n          val zis = new ZipInputStream(content.open)\n          Stream.continually(zis.getNextEntry).\n                takeWhile(_ != null).\n                flatMap { _ =>\n                    val br = new BufferedReader(new InputStreamReader(zis))\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\n                }\n    }\ntextRDD.take(1)","dateUpdated":"2018-12-20T09:42:35+0000","dateFinished":"2018-12-20T09:42:39+0000","dateStarted":"2018-12-20T09:42:35+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\ntextRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[23] at flatMap at <console>:46\nres21: Array[String] = Array(806754250\t20171201\t201712\t2017\t2017.9068\t\t\t\t\t\t\t\t\t\t\tUSA\tUNITED STATES\tUSA\t\t\t\t\t\t\t\t1\t100\t100\t10\t3\t-5.0\t4\t1\t4\t-0.66666666666667\t0\t\t\t\t\t\t\t\t3\tNew Haven, Connecticut, United States\tUS\tUSCT\t\t41.3082\t-72.9282\t209231\t3\tNew Haven, Connecticut, United States\tUS\tUSCT\t\t41.3082\t-72.9282\t209231\t20181201000000\thttps://bismarcktribune.com/news/national/the-latest-immigrant-s-supporters-end-courthouse-protest/article_76d71ef6-d0e0-5e02-ab72-4c8a0d31778e.html)\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-84-228.ec2.internal:4040/jobs/job?id=2"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545299727097_-21600987","id":"20181220-095527_363379485","dateCreated":"2018-12-20T09:55:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2224","text":"case class Event(\nGLOBALEVENTID: Int,\nSQLDATE: Int,\nMonthYear: Int,\nYear: Int,\nFractionDate: Double,\nActor1Code: String,\nActor1Name: String,\nActor1CountryCode: String,\nActor1KnownGroupCode: String,\nActor1EthnicCode: String,\nActor1Religion1Code: String,\nActor1Religion2Code: String,\nActor1Type1Code: String,\nActor1Type2Code: String,\nActor1Type3Code: String,\nActor2Code: String,\nActor2Name: String,\nActor2CountryCode: String,\nActor2KnownGroupCode: String,\nActor2EthnicCode: String,\nActor2Religion1Code: String,\nActor2Religion2Code: String,\nActor2Type1Code: String,\nActor2Type2Code: String,\nActor2Type3Code: String,\nIsRootEvent: Int,\nEventCode: String,\nEventBaseCode: String,\nEventRootCode: String,\nQuadClass: Int,\nGoldsteinScale: Double,\nNumMentions: Int,\nNumSources: Int,\nNumArticles: Int,\nAvgTone: Double,\nActor1Geo_Type: Int,\nActor1Geo_FullName: String,\nActor1Geo_CountryCode: String,\nActor1Geo_ADM1Code: String,\nActor1Geo_ADM2Code: String,\nActor1Geo_Lat: Double,\nActor1Geo_Long: Double,\nActor1Geo_FeatureID: String,\nActor2Geo_Type: Int,\nActor2Geo_FullName: String,\nActor2Geo_CountryCode: String,\nActor2Geo_ADM1Code: String,\nActor2Geo_ADM2Code: String,\nActor2Geo_Lat: Double,\nActor2Geo_Long: Double,\nActor2Geo_FeatureID: String,\nActionGeo_Type: Int,\nActionGeo_FullName: String,\nActionGeo_CountryCode: String,\nActionGeo_ADM1Code: String,\nActionGeo_ADM2Code: String,\nActionGeo_Lat: Double,\nActionGeo_Long: Double,\nActionGeo_FeatureID: String,\nDATEADDED: BigInt,\nSOURCEURL: String\n    )","dateUpdated":"2018-12-20T10:10:14+0000","dateFinished":"2018-12-20T10:10:16+0000","dateStarted":"2018-12-20T10:10:14+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Event\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545300830547_-907168479","id":"20181220-101350_2142691685","dateCreated":"2018-12-20T10:13:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2630","text":"val cachedEvents = textRDD //.cache // RDD","dateUpdated":"2018-12-20T10:42:22+0000","dateFinished":"2018-12-20T10:42:22+0000","dateStarted":"2018-12-20T10:42:22+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"cachedEvents: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[23] at flatMap at <console>:46\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545300614759_-1103841107","id":"20181220-101014_1211351157","dateCreated":"2018-12-20T10:10:14+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2430","text":"def toDouble(s : String): Double = if (s.isEmpty) 0 else s.toDouble\ndef toInt(s : String): Int = if (s.isEmpty) 0  else s.toInt\ndef toBigInt(s : String): BigInt = if (s.isEmpty) BigInt(0) else BigInt(s)\n\ncachedEvents.map(_.split(\"\\t\")).filter(_.length==61).map(\n    e=> Event(\n        toInt(e(0)),toInt(e(1)),toInt(e(2)),toInt(e(3)),toDouble(e(4)),e(5),e(6),e(7),e(8),e(9),e(10),e(11),e(12),e(13),e(14),e(15),e(16),e(17),e(18),e(19),e(20),\n        e(21),e(22),e(23),e(24),toInt(e(25)),e(26),e(27),e(28),toInt(e(29)),toDouble(e(30)),toInt(e(31)),toInt(e(32)),toInt(e(33)),toDouble(e(34)),toInt(e(35)),e(36),e(37),e(38),e(39),toDouble(e(40)),\n        toDouble(e(41)),e(42),toInt(e(43)),e(44),e(45),e(46),e(47),toDouble(e(48)),toDouble(e(49)),e(50),toInt(e(51)),e(52),e(53),e(54),e(55),toDouble(e(56)),toDouble(e(57)),e(58),toBigInt(e(59)),e(60))\n\n).toDS.createOrReplaceTempView(\"export\")\n spark.catalog.cacheTable(\"export\")","dateUpdated":"2018-12-20T10:42:25+0000","dateFinished":"2018-12-20T10:42:27+0000","dateStarted":"2018-12-20T10:42:25+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"toDouble: (s: String)Double\ntoInt: (s: String)Int\ntoBigInt: (s: String)BigInt\n"}]}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545300748598_142576470","id":"20181220-101228_1101864648","dateCreated":"2018-12-20T10:12:28+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2533","text":"spark.sql(\"\"\" SELECT * FROM export LIMIT 10\"\"\" ).show","dateUpdated":"2018-12-20T10:42:37+0000","dateFinished":"2018-12-20T10:46:22+0000","dateStarted":"2018-12-20T10:42:37+0000","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 12, ip-172-31-92-43.ec2.internal, executor 1): java.io.InterruptedIOException: getFileStatus on s3a://fabien-mael-telecom-gdelt2018/20181201000000.export.CSV.zip: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:125)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790)\n\tat org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n\tat $anonfun$1.apply(<console>:48)\n\tat $anonfun$1.apply(<console>:46)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n\tat org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1163)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1109)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:758)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:732)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:714)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:674)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:656)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:520)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4443)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4390)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1280)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1254)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 60 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:313)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:279)\n\tat sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n\tat com.amazonaws.http.conn.$Proxy11.get(Unknown Source)\n\tat org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:191)\n\tat org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n\tat org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n\tat com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1285)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1101)\n\t... 72 more\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1803)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1791)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1790)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1790)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:871)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:871)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2024)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1973)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1962)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:682)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n  at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n  at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)\n  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)\n  ... 58 elided\nCaused by: java.io.InterruptedIOException: getFileStatus on s3a://fabien-mael-telecom-gdelt2018/20181201000000.export.CSV.zip: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:125)\n  at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1571)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n  at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:790)\n  at org.apache.spark.input.PortableDataStream.open(PortableDataStream.scala:183)\n  at $anonfun$1.apply(<console>:48)\n  at $anonfun$1.apply(<console>:46)\n  at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n  at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337)\n  at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094)\n  at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085)\n  at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020)\n  at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085)\n  at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811)\n  at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:286)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n  at org.apache.spark.scheduler.Task.run(Task.scala:109)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n  ... 3 more\nCaused by: com.amazonaws.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleRetryableException(AmazonHttpClient.java:1163)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1109)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:758)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:732)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:714)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:674)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:656)\n  at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:520)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4443)\n  at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4390)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1280)\n  at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1254)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n  at org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n  ... 60 more\nCaused by: org.apache.http.conn.ConnectionPoolTimeoutException: Timeout waiting for connection from pool\n  at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.leaseConnection(PoolingHttpClientConnectionManager.java:313)\n  at org.apache.http.impl.conn.PoolingHttpClientConnectionManager$1.get(PoolingHttpClientConnectionManager.java:279)\n  at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at com.amazonaws.http.conn.ClientConnectionRequestFactory$Handler.invoke(ClientConnectionRequestFactory.java:70)\n  at com.amazonaws.http.conn.$Proxy11.get(Unknown Source)\n  at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:191)\n  at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n  at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n  at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n  at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1285)\n  at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1101)\n  ... 72 more\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-84-228.ec2.internal:4040/jobs/job?id=6"],"interpreterSettingId":"spark"}}},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1545300980738_-1250918753","id":"20181220-101620_362533774","dateCreated":"2018-12-20T10:16:20+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2785"}],"name":"ProjetINF728","id":"2E22NHP5F","noteParams":{},"noteForms":{},"angularObjects":{},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}